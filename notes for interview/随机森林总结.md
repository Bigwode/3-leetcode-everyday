# 随机森林算法 (RandonForest)

在集成学习(Ensemble Learning)中，主要分为bagging算法和boosting算法。（随机森林属于bagging算法。）

(1)、bagging的算法过程如下：

- 从原始样本集中使用Bootstraping方法(重复抽样技术从原始样本中随机抽取一定数量的样本)随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）
- 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）

(2)、boosting的算法过程如下：

- 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。

- 进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）

  Bagging，Boosting的主要区别

- 样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。

- 样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。

- 预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。

- 并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。

- 下面是将决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

https://blog.csdn.net/qq547276542/article/details/78304454

### AdaBoost算法：

AdaBoost 是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器，即弱分类器，然后把这些弱分类器集合起来，构造一个更强的最终分类器。

算法本身是改变数据分布实现的，它根据每次训练集之中的每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改权值的新数据送给下层分类器进行训练，然后将每次训练得到的分类器融合起来，作为最后的决策分类器。

**随机森林的构建过程大致如下：**

从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集
对于n_tree个训练集，我们分别训练n_tree个决策树模型
对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂。
每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝。

将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果

------

# 图像的腐蚀与膨胀

**结构元素**：设两幅图像A、B，若A是被处理对象，B是用来处理A的，那么则把B称为结构元素。

① 膨胀
⑴ 用结构元素B，扫描图像A的每一个像素
⑵ 用结构元素与其覆盖的二值图像做“或”操作
⑶ 如果有一个元素为0，结果图像的该像素为0，否则为255（如果B覆盖A的区域有一个点A为黑色对应的点B也为黑色，则该扫描点为黑色，否则为白色。）

② 腐蚀
⑴ 用结构元素B，扫描图像A的每一个像素
⑵ 用结构元素与其覆盖的二值图像做“与”操作
⑶ 如果都为0，结果图像的该像素为0，否则为255（如果结构元素B为黑色的点，图像A相对应的点都为黑色，则该点的像素为黑色，否则为白色。）