# 深度学习常用优化器总结

### 1、**Adam　**其不是首字母缩写，也不是人名，而是来源于适应性矩估计(adaptive moment estimation) 

Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而**为不同的参数设计独立的自适应性学习率**。
$$
\alpha _t = \alpha \cdot \sqrt{1-\beta _{2}^{t}}/(1-\beta _{1}^{t})
$$

$$
\theta _t = \theta _{t-1} - \alpha _t\cdot m_t/(\sqrt{\nu _t}+\hat{\epsilon })
$$

其中，$m_t$: 梯度的指数移动均值；

$\nu _t$:平方梯度；

而$\beta _{1}$ 和$\beta _{2}$ 控制了这些移动均值（moving average）的指数衰减率

[1]、https://www.jiqizhixin.com/articles/2017-07-12









https://zhuanlan.zhihu.com/p/32230623