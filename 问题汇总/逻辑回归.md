### 逻辑回归（Logistic Regression）

logistic回归可用于二分类，也可用于多分类，但是二分类更为常用，也更加容易解释，多类可以使用softmax方法进行处理。本文记录的也只是关于二分类的部分。

（１）预测函数h使用sigmoid函数，即，
$$
g(z) = \frac{1}{1+e^{-z}}　(1)
$$
（２）确定决策边界，
$$
z = \theta_0+\theta _1x_1 +...+\theta _n x_n = \sum_{i=1}^{n}\theta _ix_i=\theta ^Tx　(2)
$$
由sigmoid函数图像可知，z>0时, g(z)=1;z<0时, g(z)=0.

（３）构造损失函数.

由此可得预测函数，
$$
h_\theta (x)=g(\theta ^ T x)=\frac{1}{1+e^{-\theta^Tx}} 　　(3)
$$
$h_\theta(x)$ 代表的是结果取１的概率，所以有，
$$
\left\{\begin{matrix}
P(y=1|x;\theta)=h_\theta (x)
\\ 
P(y=0|x;\theta)= 1-h_\theta (x)
\end{matrix}\right.   (4)
$$
(4)式可变形为，
$$
P(y|x;\theta) = [h_\theta (x)]^y[1-h_\theta (x)]^{1-y}        (5)
$$
我们要估计参数$\theta$ 使得整个概率最大。使用极大似然估计

对(5)式取似然函数可得，
$$
L(\theta )=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^{m}(h_\theta (x^{(i)}))^{y^{(i)}}(1-h_\theta (x^{(i)}))^{1-y^{(i)}}　　　(6)
$$
其对数似然函数为，
$$
log(L(\theta ))=\sum_{i=1}^{m}
y^{(i)}logh_\theta (x^{(i)})+(1-y^{(i)})log[1-h_\theta (x^{(i)})]　　　(7)
$$
这里可以使用梯度上升法求使得log(L($\theta$))最大时的$\theta$值。但是这里我们令，
$$
J(\theta )=-\frac{1}{m}logL(\theta )　　　(8)
$$
这样就可以使用梯度下降法来求使得J($\theta$)最小值时的$\theta$值。

则我们参数$\theta$更新策略为，
$$
\theta _j:=\theta _j-\alpha \frac{\partial }{\partial \theta _j}J(\theta )　　(9)
$$

$$
\begin{align*}
 \frac{\partial }{\partial \theta _j}J(\theta) &= -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{1}{h_\theta (x^{(i)})}\frac{\partial }{\partial \theta _j}h_\theta (x^{(i)})-[1-y^{(i)}]\frac{1}{1-h_\theta (x^{(i)})}\frac{\partial }{\partial \theta _j}h_\theta (x^{(i)})]\\
 &= -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}(1-g(\theta ^Tx^{(i)}))-(1-y^{(i)})g(\theta ^Tx^{(i)})] \\
 &= -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}-h_\theta (x^{(i)})]x_{j}^{(i)}  \\
 &= \frac{1}{m}\sum_{i=1}^{m}[h_\theta (x^{(i)}) - y^{(i)}]x_{j}^{(i)}
\end{align*}　　　(10)
$$

为方便实现，我们可以将以上过程向量化(Vectorization)，这里就不写了。

